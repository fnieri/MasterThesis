\section{$\Delta$Q}

Originally, $\Delta$Q(x) denotes the probability that an outcome occures in a time $t \le x$, thus defining an "intangible mass" of such IRV as $1 - \lim_{x\to\infty} \Delta Q (x)$.
We then extend the original definition to fit real time constraints, needing to calculate $\Delta$Qs continuously.

For a given observable, $\Delta$Q($t_l$, $t_u$, $dMax$) is the probability that between time $t_l < t_u$, an observable occurs in time t $\le$ dMax.


\subsection{Samples}
    Given an event $e$ with its start event $e_s$ and an end event $e_e$ in an observable $o$, a sample contains:
    \begin{itemize}
        \item The start time $t_s$
        \item The end time $t_e$
        \item Its status 
        \item Its elapsed time of execution
    \end{itemize}
    The sample has three possible statuses: \texttt{success, timeout, failure}, it can thus be broken down in three possible states, based on its status:
    \begin{itemize}
        \item \textbf{($t_s$,$t_e$)}: This representation indicates that the execution was successful (t $<$ dMax), the sample contains the start time and end time of the event.
        \item \textbf{($t_s, \mathcal{T}$)}: This representation indicates that the execution has timed out (t $>$ dMax). The samples does not contain the end time, it is irrelevant as we know that its execution has taken more than dMax, thus, its execution was ended prematurely in the stub.
            \item \textbf{($t_s, \mathcal{F}$)}: This representation indicates the execution has failed given a user defined requirement (i.e. a dropped message given buffer overload in a queue system). It must not be confused with a program failure (crash), if a program crashes during the execution of event $e$, it will timeout since the stub will not receive an end message. As was the case for the timeout, we do not need to know the end time as the sample encodes a failure.
    \end{itemize}

\subsection{Internal representation of a $\Delta$Q}

        Inside our $\Delta$Q class we store the probability density function (PDF) and its empirical cumulative distribution function (ECDF) given $n$ samples. 
    \subsubsection{PDF}
  We approximate the PDF of the observed random variable $\textbf{X}$ via a histogram. We partition the values into $N$ bins of equal width, this is required to ease future calculations.
        Given $\lbrack x_i, x_{i+1} \rbrack$ the interval of a bin $i$, where $x_i = i\Delta x$, and $\hat{p}(x_i)$ the value of the PDF at bin $i$:
        \begin{equation}
            \hat{p}(x_i) = \dfrac{n_i}{n}
            \label{eq:pdf}
        \end{equation}
   With $n_i$ the number of successful samples whose elapsed time is contained in the bin $i$, $n$ the total number of samples.
\subsubsection{ECDF}
    The ECDF $\hat{f}(x_i)$ at bin $i$ can  be calculated as:
    \begin{equation}
            \hat{f}(x_i) = \sum_{j=0}^{i} \hat{p}(x_j)
        \label{eq:cdf}
    \end{equation}

\subsection{dMax}
Setting a maximum delay for an observable is not a job that can be done one-off and blindly, it is something that is done with an underlying knowledge of the system inner-workings and must be thoroughly fine tuned. 

We define in our oscilloscope a formula to dynamically define a maximum delay based on the formula:
\begin{equation}
    dMax = \Delta_{t base} * 2^n * N  
    \label{eq:dMax}
\end{equation}
Where:
\begin{itemize}
    \item $\Delta_{t base}$ represents the base width of a bin.
    \item $N$ the number of bins.
\end{itemize}


Some tradeoffs must though be acknowledged, a higher number of bins corresponds to a higher number of calculations, a lower dMax may correspond to more failures. These are all tradeoffs that must be considered by the system engineer.

\subsection{Operations}

There are a few operations that can be applied to a $\Delta$Q and between two $\Delta$Qs.

        \subsubsection{Convolution}
        
        \paragraph{Naïve convolution}
        Given two $\Delta$Q binned PDFs $f$ and $g$, the result of the convolution $f * g$ is given by 
        \begin{equation}
            (f * g)\lbrack n \rbrack = \sum_{m = 0}^{N} = f\lbrack m \rbrack g \lbrack [n - m] \rbrack  
            \label{eq:discconv}
        \end{equation}

        The problem with the naïve convolution is in its complexity, the cost of such operations is $\mathcal{O}(n^2)$, this becomes a problem quickly when you have multiple probes and need to update them quickly. 

        \paragraph{Fast Fourier Transform Convolution}

        Convolution leveraging Fast Fourier Transform (FFT) allows us to reduce the amount of calculations to $\mathcal{O}(n \text{log} n)$ 
