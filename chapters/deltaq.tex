\section{$\Delta$Q}

Originally, $\Delta$Q(x) denotes the probability that an outcome occures in a time $t \le x$, thus defining an "intangible mass" of such IRV as $1 - \lim_{x\to\infty} \Delta Q (x)$.
We then extend the original definition to fit real time constraints, needing to calculate $\Delta$Qs continuously.

For a given observable, $\Delta$Q($t_l$, $t_u$, $dMax$) is the probability that between time $t_l < t_u$, an observable occurs in time t $\le$ dMax.

\subsection{Internal representation of a $\Delta$Q}

        Inside our $\Delta$Q class we store the probability density function (PDF) and its empirical cumulative distribution function (ECDF) given $n$ samples. 
    \subsubsection{PDF}
  We approximate the PDF of the observed random variable $\textbf{X}$ via a histogram. We partition the values into $N$ bins of equal width, this is required to ease future calculations.
        Given $\lbrack x_i, x_{i+1} \rbrack$ the interval of a bin $i$, where $x_i = i\Delta x$, and $\hat{p}(x_i)$ the value of the PDF at bin $i$:
        \begin{equation}
            \hat{p}(x_i) = \dfrac{n_i}{n}
            \label{eq:pdf}
        \end{equation}
   With $n_i$ the number of successful samples whose elapsed time is contained in the bin $i$, $n$ the total number of samples.
\subsubsection{ECDF}
    The ECDF $\hat{f}(x_i)$ at bin $i$ can  be calculated as:
    \begin{equation}
            \hat{f}(x_i) = \sum_{j=0}^{i} \hat{p}(x_j)
        \label{eq:cdf}
    \end{equation}

\subsection{dMax}
Setting a maximum delay for an observable is not a job that can be done one-off and blindly, it is something that is done with an underlying knowledge of the system inner-workings and must be thoroughly fine tuned. 

We define in our oscilloscope a formula to dynamically define a maximum delay based on the formula:
\begin{equation}
    dMax = \Delta_{t base} * 2^n * N  
    \label{eq:dMax}
\end{equation}
Where:
\begin{itemize}
    \item $\Delta_{t base}$ represents the base width of a bin.
    \item $N$ the number of bins.
\end{itemize}


Some tradeoffs must though be acknowledged, a higher number of bins corresponds to a higher number of calculations, a lower dMax may correspond to more failures. These are all tradeoffs that must be considered by the system engineer.

\subsection{Operations}

There are a few operations that can be applied to a $\Delta$Q and between two $\Delta$Qs.

        \subsubsection{Convolution}
        
        \paragraph{Naïve convolution}
        Given two $\Delta$Q binned PDFs $f$ and $g$, the result of the convolution $f * g$ is given by 
        \begin{equation}
            (f * g)\lbrack n \rbrack = \sum_{m = 0}^{N} = f\lbrack m \rbrack g \lbrack [n - m] \rbrack  
            \label{eq:discconv}
        \end{equation}

        The problem with the naïve convolution is in its complexity, the cost of such operations is $\mathcal{O}(n^2)$, this becomes a problem quickly when you have multiple probes and need to update them quickly. 

        \paragraph{Fast Fourier Transform Convolution}

        Convolution leveraging Fast Fourier Transform (FFT) allows us to reduce the amount of calculations to $\mathcal{O}(n \text{log} n)$ 
